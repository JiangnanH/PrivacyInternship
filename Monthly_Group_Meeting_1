Hello every one, I'm Jiangnan. And I'm working with Rafael on this topic about privacy. It's glad to be here to give you a short presentation of our works.

Our topic is Privacy on synthetic data: metrics, attacks, defenses and measures. 


Also, we are working with professor isabelle Guyon from Inria, professor Kritin Bennett and PhD student Joseph Pedersen from RPI the Rensselaer Polytechnic Institute in US.

firstly let me give you an overview of the problem and define our objectives . Nowadays with the increasing concern for users data privacy, one of the important problem raised is the release of sensitive datasets while protecting the privacy of individuals in such datasets.

Recentely, progress in generative machine learning models has provided a new path to sharing of such data. The idea is that, instead of using the original dataset which need to be protected, we train a generator on it, then use this generator to generate another dataset which is the synthetic one. Then this new dataset is considered Privacy-preserving as they are not the real data.

This Idea sounds great, however,recent work has demonstrated that even such approaches can also cause privacy leakage, since the generator is trained on the original dataset, so the adversaries can still infer some private information of the original data from the generated one with some query access to the synthetic data outputs. The situation are even worse when the ganerators are overfit to their training datasets. 


So here comes our objectives:

Of course our final goal is to protect the privacy of the original dataset. To achieve this final goal, we got three small objectives, the first one is that we'd like to realized an attack against privacy by ourself on generative models ,and if have time also on predictive models, base on some litteratures. then analyse the information leakage from these models to undertand the pricipal of these attacks and to see how serious the situation could be.

The second objective is that when we actually have an original dataset and create a synthetic dataset from it, we want to know how much information are leaked, so we need to find or created a method to measure the privacy and utility of the synthetic data.
And to anlyse the trade off between the utility and the privacy.


After understand the pricipal of the attacks and can well measure the privacy and utility of the synthetic data, we now want to actually do some thing to reduce the privacy leakage from the original dataset. So our third objective is to find or create methods to protect the privacy. But as time is limit, we won't pay muc attention on this objective.


Aftering definding our objectives, I'd like to do a second introduction about some more specific stuffs, some terms. The first thing I'm gonna present is the attacks. As I've say that we 'are going to make our own attack on the original private dataset, but in fact there exists many kinds of different attack in the field of machine learning. Amont these different kinds of attacks, the two most basic and inportants ones are membership inference attack and attribute inference attack. And our work will mainly focus on the membership inference attack.

What is a membership inference attack? This figure give a simple senarios of this kind of attack, firstly there is a model trained on an training dataset, the model could be a predictive model like a classifier or a generative model like GANs. In this figure it is a classifier. The attacker who do a membership inference attack wants to know whether a specific record was in the training set of this given model or not. And the attacker has some access to this model. For example if it's a predictive model then he can use the record  as an input to the model and have its prediction vectors. If it's a generative model, he can use it to generate new data, so in this way the attacker can somehow gather some informations.

Then I'll also give you a remaind of GANs, the Generative  Adversarial  Networks.

GAns  are now the one of the most  common used  generative  models. The GAN  algorithm learns a distribution of a dataset by adversarially training two modules, which are a generator and a discriminator. The goal of the generator is to learn a transformation that would convert a random noise to a realistic data sample. The goal of the discriminator is distinguish synthetic samples (generated by the generator) from real samples. In our work, we do our experiments with Wasserstain-GAN which is one of the variations of the original GAN.

So now we have the GANs, we can create our own synthetic dataset.

But Once we have the synthetic data, then how to measure them? here I put some synthetic bedroom images generated by using the GAN. We as humain, we can say intuitively that the quality of the bedroom images are quilt good. But humain's judgment is actually subjective, and expensive, cause we need to orgnize lot of people to provide their own opinion for a signl experiment. and of cause, we can barely figure out the privacy leakage by just looking at this pictures. So we need some methods to quantitatively measure the quality of the synthetic data. 

In fact even in the original paper of this picture, they just said that they 'believe' these synthetic bedrooms are comparable.

The quality of the synthetic data can contains many aspects, for example the quality of the synthetic samples themselves, which means does each of the synthetic sample looks like a real sample. The diversity of the data, which means weither the Generator can generate samples of different types. if the model always generate certain type of samples, then we say it's a mode collapse. 

The first two terms, the samples' quality and the diversity can be considered as one term which is 'utility' of the synthetic data. Cause if we wish to reuse the synthetic data as surrogate of the original data, we would like to have some synthedic data looks like the real one and diverse enough.

Then of cause the synthetic data has privacy as their quality, cause we're interested in the information leakage. In many currents works, The privacy and the utility are considered as a trade-off. That's because when we want to protect the privacy of the original dataset, we need to somehow hide some private information of it, then the synthetic data generated without these information usually leads to a decrease of utility.



There are also other qualities of synthetic data like faireness and resemblance. But our work will mainly focus on the privacy and utility.


Now Rafael will present you the Material and the Methode than we are working with.



Ok thanks you rafael! so apart from the works that we did together, my work is more focus on the measures of the quality of the synthetic datas.


Let'me first show you some Prior art of the measures of synthetic data, infact, there exist already a survey paper on GAN evaluation measures, which contains most of the measures proposed in th recent years. As how to measure the synthetia data is still an ongoing topic, the measures proposed varies alot, some people, you know, when they create a new architecture of GANs, they may proposed a measures to prove that their generator is good. And one of the most widly use measures is FID, many people use this measure to benchmark their model. 

The idea of FID is that we embeds a set of real samples and a set of generated samples into a feature space given by a specific layer of a neural network.(in the original FID they use Inception Network, but it can be replaced by any other one). Then it supposed the embedding feature space as a continuous multivariate Gaussian, the mean and covariance are estimated for both the generated data and the real data. 

Also, for the measure of membership privacy of synthetic data, recently I discover this paper where some researchers provide a framework named MACE, (which is abbrivation of Membership privacy estimation) But I haven't go into the detials of this work yet. However by using this frame work, one can provide the Original data and the Non-traning data with the Generative model used, and the query function with the Metric which could be used by the attacker. Then the framework can provide the Estimated membership privacy loss with thier confidence interval. For instance, The output would be like this. So with this framework we can quantitatively measure the membership privacy loss of a generative model and its training set. 

And if we combine this method, with the first one using for measure the utility of the synthetic data, then we can qualitatively anlayse the trade-off between the utility and the membership privacy.

Here are some difficuties in the topic of measuring the synthetic data. The first and most basic challenge is that the synthetic datas are hard to measure. This is just like doing unsupervise learning, we don't have labels for each record then we can't do a simple calculate and then have some thing like the precision, the recall, but we have to think about other way to approximate this kind of estimation. And this is also why it is interesting, cause with no standard answer, every one can provide thier own idea than they considered appropriate.

The second challenges is that some measures are very complex and time consuming, some times some measures can even take more time than training processing of the generative model. so when design a measure, the efficiency should also be considered. The efficiency here contains time efficiency and sample efficiency. the first one means the average time consumed for evaluate one sigal record, the second one means means the number of samples a measure need to provide a evaluation with high confidence.

Also, as there exist many kinds of measures, how to select suitable measures for a specific task is also one of the difficulties.

 
Now I'll talk about the preliminary results:

Here I use the WGAN trained on the preprocessed Nist dataset that Rafael has presented,  to generate some synthetic data during it's training process from 1000 iterations to 10000 iterations. we can see that the quality of the digits become better. the shape of the digits become smoother and their contours are getting more and more clear. Well at least is easy to see at the beginning of the training process, but as the training iterations increases, it become harder to see the difference of the synthetic data.

Then I reproduced the FID measures and try to estimate the synthetic methode, then we have some specific numbers of distance between these synthetic datas to the original ones, the utility of the synthetic data can then be indicated qualitatively by these numbers.

Then I plot the changes of the FID distance during the training process of GAN.The figure at left. Here the horizontal axis indicate number of thousand of iterations, and vertical axis is the FID distance. And each time when I estimate the distance I randomly extracted 10,000 samples from the original dataset and the synthetis dataset to do the computation, no we can clearly see that the distance between the synthetic data and original data become closer and closer and have a tendence to converge.

And the figure at right represent that, after doing 10,000 iterations, I iteratelly extracte 1000 samples, 2000 samples and 3000, utill all the samples are use to estimate the FID distance. so the horizonal axis indicate the number of thousands of samples that I extracted. 

Then we can observe with the same iterations level, that when we more and more samples are used, the FID Distance decreased. That may indicate that their are some kind of bias when using this metric to evaluate the synthetic data. In one hands, this may cause sample in-efficience, which means we need a lot of sample to get a evalutaion with high confidence. But on the other hand, this means FID can be use to detect mode collapse. Cause when our model fail to generate diverse samples, then it can't be able to get a good grade with this metric.


Now Rafael will present you about his progress.


