{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.13"
    },
    "colab": {
      "name": "resnet_nist_train_classifier.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "wlmOOCfA9eOW",
        "bE6D6Nmp9Xvd",
        "I-WTN7yqi_Im",
        "XJfsE5D8RYJe",
        "r1bbja_4RccP",
        "Wx8r-tf30g0k"
      ],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPAUk23Sddpc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "932e5bda-6862-4cd1-e5ea-b824e41e41fd"
      },
      "source": [
        "!pip install tensorflow-gpu==1.15.0\n",
        "!pip install h5py=='2.9.0'"
      ],
      "id": "iPAUk23Sddpc",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu==1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/72/d06017379ad4760dc58781c765376ce4ba5dcf3c08d37032eeefbccf1c51/tensorflow_gpu-1.15.0-cp37-cp37m-manylinux2010_x86_64.whl (411.5MB)\n",
            "\u001b[K     |████████████████████████████████| 411.5MB 39kB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (1.15.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (0.36.2)\n",
            "Collecting keras-applications>=1.0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 6.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (0.8.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (1.1.0)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/e9/d3d747a97f7188f48aa5eda486907f3b345cd409f0a0850468ba867db246/tensorboard-1.15.0-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 31.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (0.12.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (1.19.5)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (0.2.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (1.34.1)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/62/2ee9cd74c9fa2fa450877847ba560b260f5d0fb70ee0595203082dafcc9d/tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503kB)\n",
            "\u001b[K     |████████████████████████████████| 512kB 35.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (1.12.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (1.1.2)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (3.12.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (3.3.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu==1.15.0) (3.1.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.0) (56.1.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.0) (3.3.4)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.0) (1.0.1)\n",
            "Requirement already satisfied: cached-property; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow-gpu==1.15.0) (1.5.2)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.0) (4.0.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.0) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.0) (3.7.4.3)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp37-none-any.whl size=7540 sha256=be082dbe6dc39b2d1819588b6f3c7426b6de02d7060594d47432c00b392f8cdc\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built gast\n",
            "\u001b[31mERROR: tensorflow 2.5.0 has requirement gast==0.4.0, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.5.0 has requirement tensorboard~=2.5, but you'll have tensorboard 1.15.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.5.0 has requirement tensorflow-estimator<2.6.0,>=2.5.0rc0, but you'll have tensorflow-estimator 1.15.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow-probability 0.12.1 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "Installing collected packages: keras-applications, tensorboard, gast, tensorflow-estimator, tensorflow-gpu\n",
            "  Found existing installation: tensorboard 2.5.0\n",
            "    Uninstalling tensorboard-2.5.0:\n",
            "      Successfully uninstalled tensorboard-2.5.0\n",
            "  Found existing installation: gast 0.4.0\n",
            "    Uninstalling gast-0.4.0:\n",
            "      Successfully uninstalled gast-0.4.0\n",
            "  Found existing installation: tensorflow-estimator 2.5.0\n",
            "    Uninstalling tensorflow-estimator-2.5.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.5.0\n",
            "Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-1.15.0 tensorflow-estimator-1.15.1 tensorflow-gpu-1.15.0\n",
            "Collecting h5py==2.9.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8e/fd/2ca5c4f4ed33ac4178f9c4d551e3946ab480866e3cd67a65a67a4bb35367/h5py-2.9.0-cp37-cp37m-manylinux1_x86_64.whl (2.8MB)\n",
            "\u001b[K     |████████████████████████████████| 2.8MB 3.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.7/dist-packages (from h5py==2.9.0) (1.19.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py==2.9.0) (1.15.0)\n",
            "\u001b[31mERROR: tensorflow 2.5.0 has requirement gast==0.4.0, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.5.0 has requirement h5py~=3.1.0, but you'll have h5py 2.9.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.5.0 has requirement tensorboard~=2.5, but you'll have tensorboard 1.15.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.5.0 has requirement tensorflow-estimator<2.6.0,>=2.5.0rc0, but you'll have tensorflow-estimator 1.15.1 which is incompatible.\u001b[0m\n",
            "Installing collected packages: h5py\n",
            "  Found existing installation: h5py 3.1.0\n",
            "    Uninstalling h5py-3.1.0:\n",
            "      Successfully uninstalled h5py-3.1.0\n",
            "Successfully installed h5py-2.9.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYH_eNE8eCZf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d3db874-b959-40f2-c6e2-ef771c7fb653"
      },
      "source": [
        "!git clone https://github.com/JiangnanH/PrivacyInternship\n",
        "%cd /content/PrivacyInternship/wgan-gp"
      ],
      "id": "zYH_eNE8eCZf",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'PrivacyInternship'...\n",
            "remote: Enumerating objects: 263, done.\u001b[K\n",
            "remote: Counting objects: 100% (263/263), done.\u001b[K\n",
            "remote: Compressing objects: 100% (187/187), done.\u001b[K\n",
            "remote: Total 263 (delta 116), reused 217 (delta 70), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (263/263), 47.33 MiB | 35.15 MiB/s, done.\n",
            "Resolving deltas: 100% (116/116), done.\n",
            "/content/PrivacyInternship/wgan-gp\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTH4grR6d81h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a8bd184-5761-4ba6-b993-3861accdb7f1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "CTH4grR6d81h",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwLjV1zSfiHT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4e3b11d-0864-4c0f-faeb-9e079aec8780"
      },
      "source": [
        "%cd /content/PrivacyInternship/wgan-gp"
      ],
      "id": "HwLjV1zSfiHT",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/PrivacyInternship/wgan-gp\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sexual-rachel"
      },
      "source": [
        "import os\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "np.random.seed(2142)\n",
        "from tensorflow import set_random_seed\n",
        "set_random_seed(2)\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.applications import ResNet50V2\n",
        "from tensorflow.keras.layers import Dense, BatchNormalization, Dropout, Convolution2D, Input,Activation, ZeroPadding2D, MaxPooling2D, Flatten, Add\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.losses import sparse_categorical_crossentropy as scc\n",
        "from tensorflow.keras.losses import binary_crossentropy as bc\n",
        "from tensorflow.keras.metrics import AUC, Precision, Recall\n",
        "from tensorflow.keras.datasets import mnist\n",
        "import tensorflow_hub as hub\n",
        "from sklearn.model_selection import train_test_split\n",
        "import cv2\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "#from sklearn import svm\n",
        "\n",
        "from tflib.nist import load_nist_images"
      ],
      "id": "sexual-rachel",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "working-asset"
      },
      "source": [
        "def load_hsf_nist(hsf_num, num_images=1000, resize_width=28, resize_height=28):\n",
        "    with open(os.path.join('/content/drive/MyDrive/Internships/data/nist', 'HSF_{}_images.npy'.format(hsf_num)),'rb') as f:\n",
        "        images = load_nist_images(np.load(f), num_images, resize_width=resize_width, resize_height=resize_height)\n",
        "    with open(os.path.join('/content/drive/MyDrive/Internships/data/nist', 'HSF_{}_labels.npy'.format(hsf_num)),'rb') as f:\n",
        "        labels = np.load(f)[:num_images]\n",
        "    images = images.reshape(num_images, resize_width, resize_height, 1)\n",
        "    return (images,labels)\n",
        "\n",
        "def load_nist_by_origin(num_images=30000, source='all', resize_width=28, resize_height=28):    \n",
        "    if source == 'all':\n",
        "        num = int(num_images/3)\n",
        "        images1,labels1 = load_hsf_nist(4, num, resize_width, resize_height)\n",
        "        images2,labels2 = load_hsf_nist(6, num, resize_width, resize_height)\n",
        "        images3,labels3 = load_hsf_nist(1, num, resize_width, resize_height)\n",
        "        output=(np.concatenate((images1,images2,images3)),np.concatenate((labels1,labels2,labels3)))\n",
        "    elif source == 'census':\n",
        "        num = int(num_images/2)\n",
        "        images2,labels2 = load_hsf_nist(6, num, resize_width, resize_height)\n",
        "        images3,labels3 = load_hsf_nist(1, num, resize_width, resize_height)\n",
        "        output=(np.concatenate((images2,images3)),np.concatenate((labels2,labels3)))\n",
        "    elif source == 'high-school':\n",
        "        num = int(num_images)\n",
        "        output = load_hsf_nist(4, num, resize_width, resize_height)\n",
        "    return output\n",
        "\n",
        "def load_high_school_vs_non(num_images=30000, resize_width=28, resize_height=28):\n",
        "    num = int(num_images/2)\n",
        "\n",
        "    images1,_ = load_hsf_nist(4, num, resize_width, resize_height)\n",
        "    images1 = images1.reshape(num,resize_width, resize_height,1)\n",
        "    labels1 = np.array([1 for l in range(images1.shape[0])]) \n",
        "       \n",
        "    images2,_ = load_hsf_nist(1, num, resize_width, resize_height)\n",
        "    images2 = images2.reshape(num,resize_width, resize_height,1)\n",
        "    labels2 = np.array([0 for l in range(images2.shape[0])])\n",
        "    output=(np.concatenate((images1,images2)),np.concatenate((labels1,labels2)))\n",
        "    return output\n",
        "\n",
        "def get_argmax_labels(preds):\n",
        "    predicted_labels=[]\n",
        "    for array in preds:\n",
        "        argmax=np.where(array == np.amax(array))[0][0]\n",
        "        predicted_labels.append(argmax)\n",
        "    return predicted_labels\n",
        "\n",
        "def grey2RGB(gray):\n",
        "    return cv2.cvtColor(gray.astype('float32'), cv2.COLOR_GRAY2BGR)"
      ],
      "id": "working-asset",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "collected-district"
      },
      "source": [
        "def get_resnet():\n",
        "    # In order to make things less confusing, all layers have been declared first, and then used\n",
        "    \n",
        "    # declaration of layers\n",
        "    input_img = Input((28, 28, 1), name='input_layer')\n",
        "    zeroPad1 = ZeroPadding2D((1,1), name='zeroPad1')\n",
        "    zeroPad1_2 = ZeroPadding2D((1,1), name='zeroPad1_2')\n",
        "    layer1 = Convolution2D(6, (3, 3), strides=(2, 2), kernel_initializer='he_uniform', name='major_conv')\n",
        "    layer1_2 = Convolution2D(16, (3, 3), strides=(2, 2), kernel_initializer='he_uniform', name='major_conv2')\n",
        "    zeroPad2 = ZeroPadding2D((1,1), name='zeroPad2')\n",
        "    zeroPad2_2 = ZeroPadding2D((1,1), name='zeroPad2_2')\n",
        "    layer2 = Convolution2D(6, (3, 3), strides=(1,1), kernel_initializer='he_uniform', name='l1_conv')\n",
        "    layer2_2 = Convolution2D(16, (3, 3), strides=(1,1), kernel_initializer='he_uniform', name='l1_conv2')\n",
        "\n",
        "\n",
        "    zeroPad3 = ZeroPadding2D((1,1), name='zeroPad3')\n",
        "    zeroPad3_2 = ZeroPadding2D((1,1), name='zeroPad3_2')\n",
        "    layer3 = Convolution2D(6, (3, 3), strides=(1, 1), kernel_initializer='he_uniform', name='l2_conv')\n",
        "    layer3_2 = Convolution2D(16, (3, 3), strides=(1, 1), kernel_initializer='he_uniform', name='l2_conv2')\n",
        "\n",
        "    layer4 = Dense(64, activation='relu', kernel_initializer='he_uniform', name='dense1')\n",
        "    layer5 = Dense(32, activation='relu', kernel_initializer='he_uniform', name='dense2')\n",
        "\n",
        "    final = Dense(10, activation='softmax', kernel_initializer='he_uniform', name='classifier')\n",
        "    \n",
        "    # declaration completed\n",
        "    \n",
        "    first = zeroPad1(input_img)\n",
        "    second = layer1(first)\n",
        "    second = BatchNormalization(axis=1, name='major_bn')(second)\n",
        "    second = Activation('relu', name='major_act')(second)\n",
        "\n",
        "    third = zeroPad2(second)\n",
        "    third = layer2(third)\n",
        "    third = BatchNormalization(axis=1, name='l1_bn')(third)\n",
        "    third = Activation('relu', name='l1_act')(third)\n",
        "\n",
        "    third = zeroPad3(third)\n",
        "    third = layer3(third)\n",
        "    third = BatchNormalization(axis=1, name='l1_bn2')(third)\n",
        "    third = Activation('relu', name='l1_act2')(third)\n",
        "\n",
        "\n",
        "    #res = merge([third, second], mode='sum', name='res')\n",
        "    res = Add(name='res')([third, second])\n",
        "\n",
        "\n",
        "    first2 = zeroPad1_2(res)\n",
        "    second2 = layer1_2(first2)\n",
        "    second2 = BatchNormalization(axis=1, name='major_bn2')(second2)\n",
        "    second2 = Activation('relu', name='major_act2')(second2)\n",
        "\n",
        "\n",
        "    third2 = zeroPad2_2(second2)\n",
        "    third2 = layer2_2(third2)\n",
        "    third2 = BatchNormalization(axis=1, name='l2_bn')(third2)\n",
        "    third2 = Activation('relu', name='l2_act')(third2)\n",
        "\n",
        "    third2 = zeroPad3_2(third2)\n",
        "    third2 = layer3_2(third2)\n",
        "    third2 = BatchNormalization(axis=1, name='l2_bn2')(third2)\n",
        "    third2 = Activation('relu', name='l2_act2')(third2)\n",
        "\n",
        "    #res2 = merge([third2, second2], mode='sum', name='res2')\n",
        "    res2 = Add(name='res2')([third2, second2])\n",
        "    res2 = Flatten(name='flatten')(res2)\n",
        "\n",
        "    res2 = layer4(res2)\n",
        "    res2 = Dropout(0.4, name='dropout1')(res2)\n",
        "    res2 = layer5(res2)\n",
        "    res2 = Dropout(0.4, name='dropout2')(res2)\n",
        "    res2 = final(res2)\n",
        "    model = Model(inputs=input_img, outputs=res2)\n",
        "\n",
        "    if binary_classification:\n",
        "        metrics = ['accuracy', AUC()]\n",
        "        loss = bc\n",
        "        final = Dense(1, activation='sigmoid', kernel_initializer='he_uniform', name='classifier')\n",
        "    else:\n",
        "        metrics = ['accuracy']\n",
        "        loss = scc\n",
        "        final = Dense(10, activation='softmax', kernel_initializer='he_uniform', name='classifier')\n",
        "    \n",
        "    \n",
        "    sgd = SGD(decay=0., lr=0.01, momentum=0.9, nesterov=True)\n",
        "    model.compile(loss=scc, optimizer=sgd, metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def get_resnet_binary():\n",
        "    # In order to make things less confusing, all layers have been declared first, and then used\n",
        "    \n",
        "    # declaration of layers\n",
        "    input_img = Input((28, 28, 1), name='input_layer')\n",
        "    zeroPad1 = ZeroPadding2D((1,1), name='zeroPad1')\n",
        "    zeroPad1_2 = ZeroPadding2D((1,1), name='zeroPad1_2')\n",
        "    layer1 = Convolution2D(6, (3, 3), strides=(2, 2), kernel_initializer='he_uniform', name='major_conv')\n",
        "    layer1_2 = Convolution2D(16, (3, 3), strides=(2, 2), kernel_initializer='he_uniform', name='major_conv2')\n",
        "    zeroPad2 = ZeroPadding2D((1,1), name='zeroPad2')\n",
        "    zeroPad2_2 = ZeroPadding2D((1,1), name='zeroPad2_2')\n",
        "    layer2 = Convolution2D(6, (3, 3), strides=(1,1), kernel_initializer='he_uniform', name='l1_conv')\n",
        "    layer2_2 = Convolution2D(16, (3, 3), strides=(1,1), kernel_initializer='he_uniform', name='l1_conv2')\n",
        "\n",
        "\n",
        "    zeroPad3 = ZeroPadding2D((1,1), name='zeroPad3')\n",
        "    zeroPad3_2 = ZeroPadding2D((1,1), name='zeroPad3_2')\n",
        "    layer3 = Convolution2D(6, (3, 3), strides=(1, 1), kernel_initializer='he_uniform', name='l2_conv')\n",
        "    layer3_2 = Convolution2D(16, (3, 3), strides=(1, 1), kernel_initializer='he_uniform', name='l2_conv2')\n",
        "\n",
        "    layer4 = Dense(64, activation='relu', kernel_initializer='he_uniform', name='dense1')\n",
        "    layer5 = Dense(32, activation='relu', kernel_initializer='he_uniform', name='dense2')\n",
        "\n",
        "    final = Dense(1, activation='sigmoid', kernel_initializer='he_uniform', name='classifier')\n",
        "    \n",
        "    # declaration completed\n",
        "    \n",
        "    first = zeroPad1(input_img)\n",
        "    second = layer1(first)\n",
        "    second = BatchNormalization(axis=1, name='major_bn')(second)\n",
        "    second = Activation('relu', name='major_act')(second)\n",
        "\n",
        "    third = zeroPad2(second)\n",
        "    third = layer2(third)\n",
        "    third = BatchNormalization(axis=1, name='l1_bn')(third)\n",
        "    third = Activation('relu', name='l1_act')(third)\n",
        "\n",
        "    third = zeroPad3(third)\n",
        "    third = layer3(third)\n",
        "    third = BatchNormalization(axis=1, name='l1_bn2')(third)\n",
        "    third = Activation('relu', name='l1_act2')(third)\n",
        "\n",
        "\n",
        "    #res = merge([third, second], mode='sum', name='res')\n",
        "    res = Add(name='res')([third, second])\n",
        "\n",
        "\n",
        "    first2 = zeroPad1_2(res)\n",
        "    second2 = layer1_2(first2)\n",
        "    second2 = BatchNormalization(axis=1, name='major_bn2')(second2)\n",
        "    second2 = Activation('relu', name='major_act2')(second2)\n",
        "\n",
        "\n",
        "    third2 = zeroPad2_2(second2)\n",
        "    third2 = layer2_2(third2)\n",
        "    third2 = BatchNormalization(axis=1, name='l2_bn')(third2)\n",
        "    third2 = Activation('relu', name='l2_act')(third2)\n",
        "\n",
        "    third2 = zeroPad3_2(third2)\n",
        "    third2 = layer3_2(third2)\n",
        "    third2 = BatchNormalization(axis=1, name='l2_bn2')(third2)\n",
        "    third2 = Activation('relu', name='l2_act2')(third2)\n",
        "\n",
        "    #res2 = merge([third2, second2], mode='sum', name='res2')\n",
        "    res2 = Add(name='res2')([third2, second2])\n",
        "    res2 = Flatten()(res2)\n",
        "\n",
        "    res2 = layer4(res2)\n",
        "    res2 = Dropout(0.4, name='dropout1')(res2)\n",
        "    res2 = layer5(res2)\n",
        "    res2 = Dropout(0.4, name='dropout2')(res2)\n",
        "    res2 = final(res2)\n",
        "    model = Model(inputs=input_img, outputs=res2)\n",
        "    \n",
        "    \n",
        "    sgd = SGD(decay=0., lr=0.01, momentum=0.9, nesterov=True)\n",
        "    model.compile(loss=bc, optimizer=sgd, metrics=['accuracy',AUC()])\n",
        "    return model\n",
        "\n",
        "def get_resnet(binary_classification=False):     \n",
        "    # In order to make things less confusing, all layers have been declared first, and then used\n",
        "    \n",
        "    # declaration of layers\n",
        "    input_img = Input((28, 28, 1), name='input_layer')\n",
        "    zeroPad1 = ZeroPadding2D((1,1), name='zeroPad1')\n",
        "    zeroPad1_2 = ZeroPadding2D((1,1), name='zeroPad1_2')\n",
        "    layer1 = Convolution2D(6, (3, 3), strides=(2, 2), kernel_initializer='he_uniform', name='major_conv')\n",
        "    layer1_2 = Convolution2D(16, (3, 3), strides=(2, 2), kernel_initializer='he_uniform', name='major_conv2')\n",
        "    zeroPad2 = ZeroPadding2D((1,1), name='zeroPad2')\n",
        "    zeroPad2_2 = ZeroPadding2D((1,1), name='zeroPad2_2')\n",
        "    layer2 = Convolution2D(6, (3, 3), strides=(1,1), kernel_initializer='he_uniform', name='l1_conv')\n",
        "    layer2_2 = Convolution2D(16, (3, 3), strides=(1,1), kernel_initializer='he_uniform', name='l1_conv2')\n",
        "\n",
        "\n",
        "    zeroPad3 = ZeroPadding2D((1,1), name='zeroPad3')\n",
        "    zeroPad3_2 = ZeroPadding2D((1,1), name='zeroPad3_2')\n",
        "    layer3 = Convolution2D(6, (3, 3), strides=(1, 1), kernel_initializer='he_uniform', name='l2_conv')\n",
        "    layer3_2 = Convolution2D(16, (3, 3), strides=(1, 1), kernel_initializer='he_uniform', name='l2_conv2')\n",
        "\n",
        "    layer4 = Dense(64, activation='relu', kernel_initializer='he_uniform', name='dense1')\n",
        "    layer5 = Dense(32, activation='relu', kernel_initializer='he_uniform', name='dense2')\n",
        "\n",
        "    if binary_classification:\n",
        "        final = Dense(1, activation='sigmoid', kernel_initializer='he_uniform', name='classifier')\n",
        "    else:\n",
        "        final = Dense(10, activation='softmax', kernel_initializer='he_uniform', name='classifier')\n",
        "    \n",
        "    # declaration completed\n",
        "    \n",
        "    first = zeroPad1(input_img)\n",
        "    second = layer1(first)\n",
        "    second = BatchNormalization(axis=1, name='major_bn')(second)\n",
        "    second = Activation('relu', name='major_act')(second)\n",
        "\n",
        "    third = zeroPad2(second)\n",
        "    third = layer2(third)\n",
        "    third = BatchNormalization(axis=1, name='l1_bn')(third)\n",
        "    third = Activation('relu', name='l1_act')(third)\n",
        "\n",
        "    third = zeroPad3(third)\n",
        "    third = layer3(third)\n",
        "    third = BatchNormalization(axis=1, name='l1_bn2')(third)\n",
        "    third = Activation('relu', name='l1_act2')(third)\n",
        "\n",
        "\n",
        "    #res = merge([third, second], mode='sum', name='res')\n",
        "    res = Add(name='res')([third, second])\n",
        "\n",
        "\n",
        "    first2 = zeroPad1_2(res)\n",
        "    second2 = layer1_2(first2)\n",
        "    second2 = BatchNormalization(axis=1, name='major_bn2')(second2)\n",
        "    second2 = Activation('relu', name='major_act2')(second2)\n",
        "\n",
        "\n",
        "    third2 = zeroPad2_2(second2)\n",
        "    third2 = layer2_2(third2)\n",
        "    third2 = BatchNormalization(axis=1, name='l2_bn')(third2)\n",
        "    third2 = Activation('relu', name='l2_act')(third2)\n",
        "\n",
        "    third2 = zeroPad3_2(third2)\n",
        "    third2 = layer3_2(third2)\n",
        "    third2 = BatchNormalization(axis=1, name='l2_bn2')(third2)\n",
        "    third2 = Activation('relu', name='l2_act2')(third2)\n",
        "\n",
        "    #res2 = merge([third2, second2], mode='sum', name='res2')\n",
        "    res2 = Add(name='res2')([third2, second2])\n",
        "    res2 = Flatten(name='flatten')(res2)\n",
        "\n",
        "    res2 = layer4(res2)\n",
        "    res2 = Dropout(0.4, name='dropout1')(res2)\n",
        "    res2 = layer5(res2)\n",
        "    res2 = Dropout(0.4, name='dropout2')(res2)\n",
        "    res2 = final(res2)\n",
        "    model = Model(inputs=input_img, outputs=res2)\n",
        "\n",
        "    if binary_classification:\n",
        "        metrics = ['accuracy', AUC()]\n",
        "        loss = bc\n",
        "    else:\n",
        "        metrics = ['accuracy']\n",
        "        loss = scc \n",
        "    sgd = SGD(decay=0., lr=0.01, momentum=0.9, nesterov=True)\n",
        "    model.compile(loss=loss, optimizer=sgd, metrics=metrics)\n",
        "    return model"
      ],
      "id": "collected-district",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GeuH9OBt8sNK"
      },
      "source": [
        "X,y=load_nist_by_origin(80000, source='all')\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)"
      ],
      "id": "GeuH9OBt8sNK",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SefzWpzy7POP"
      },
      "source": [
        "# tfhub classifier trained with MNIST\n",
        "tfhub_classifier = hub.KerasLayer(\"https://tfhub.dev/tensorflow/tfgan/eval/mnist/logits/1\", trainable=False)\n",
        "input_vector = Input(shape=(28,28,1), name=\"images\", dtype=np.float32)\n",
        "result_vector= tfhub_classifier(input_vector)\n",
        "tfhub_classifier = Model(inputs=[input_vector], outputs=[result_vector])"
      ],
      "id": "SefzWpzy7POP",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRZujO1E8svO"
      },
      "source": [
        "# Own Resnet classifier trained with NIST\n",
        "res = get_resnet()\n",
        "history = res.fit(X_train, y_train, validation_split=0.1, verbose=2, epochs=15, batch_size=128)"
      ],
      "id": "WRZujO1E8svO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ozpUjU8z48zC",
        "outputId": "a6dc845d-cbcc-4076-a9f3-693f80cad3c7"
      },
      "source": [
        "# keras Resnet50V2 classifier pre-trained with imagenet\n",
        "resize_width = 32\n",
        "resize_height = 32\n",
        "keras_feature_extractor = ResNet50V2(weights='imagenet', include_top=False, input_shape=(resize_width,resize_height,3))"
      ],
      "id": "ozpUjU8z48zC",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://github.com/keras-team/keras-applications/releases/download/resnet/resnet50v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94674944/94668760 [==============================] - 2s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3LknsWY5fRH"
      },
      "source": [
        "X,y = load_nist_by_origin(80000, 'all', resize_width, resize_height)\n",
        "X_train_keras, X_test_keras, y_train_keras, y_test_keras = train_test_split(X, y, test_size=0.1, random_state=42)\n",
        "X_test_keras = np.apply_along_axis(grey2RGB,-1,X_test_keras).reshape(X_test_keras.shape[0], resize_width, resize_height,3)\n",
        "X_train_keras = np.apply_along_axis(grey2RGB,-1,X_train_keras).reshape(X_train_keras.shape[0], resize_width, resize_height,3)"
      ],
      "id": "J3LknsWY5fRH",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bE6D6Nmp9Xvd"
      },
      "source": [
        "## Digit recognition across different subsets"
      ],
      "id": "bE6D6Nmp9Xvd"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iOxwoldZZhG"
      },
      "source": [
        "### MNIST test set"
      ],
      "id": "4iOxwoldZZhG"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddt71In0S6Sk"
      },
      "source": [
        "(X_train_mnist, y_train_mnist), (X_test_mnist, y_test_mnist) = mnist.load_data()\n",
        "assert X_train_mnist.shape == (60000, 28, 28)\n",
        "assert X_test_mnist.shape == (10000, 28, 28)\n",
        "assert y_train_mnist.shape == (60000,)\n",
        "assert y_test_mnist.shape == (10000,)\n",
        "X_test_mnist = (X_test_mnist/254.).reshape(10000, 28, 28, 1)\n",
        "X_train_mnist = (X_train_mnist/254.).reshape(60000, 28, 28, 1)"
      ],
      "id": "ddt71In0S6Sk",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRMQLKdRTDQv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cdd55ab-86d7-4127-9df5-d82fa4c41fd9"
      },
      "source": [
        "loss_arr=[]\n",
        "acc_arr=[]\n",
        "for i in range(10):\n",
        "    res = get_resnet()\n",
        "    history = res.fit(X_train, y_train, validation_split=0.1, verbose=0, epochs=15, batch_size=128, shuffle=False)\n",
        "    loss,acc = res.evaluate(X_test_mnist,y_test_mnist)\n",
        "    loss_arr.append(loss)\n",
        "    acc_arr.append(acc)"
      ],
      "id": "QRMQLKdRTDQv",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 1s 108us/sample - loss: 1.2651 - acc: 0.6373\n",
            "10000/10000 [==============================] - 1s 109us/sample - loss: 1.3350 - acc: 0.6165\n",
            "10000/10000 [==============================] - 1s 109us/sample - loss: 1.2656 - acc: 0.6320\n",
            "10000/10000 [==============================] - 1s 111us/sample - loss: 1.4701 - acc: 0.6243\n",
            "10000/10000 [==============================] - 1s 113us/sample - loss: 1.1771 - acc: 0.7317\n",
            "10000/10000 [==============================] - 1s 117us/sample - loss: 1.3108 - acc: 0.6612\n",
            "10000/10000 [==============================] - 1s 125us/sample - loss: 1.1801 - acc: 0.6520\n",
            "10000/10000 [==============================] - 1s 121us/sample - loss: 1.3733 - acc: 0.6897\n",
            "10000/10000 [==============================] - 1s 122us/sample - loss: 2.0206 - acc: 0.5935\n",
            "10000/10000 [==============================] - 1s 130us/sample - loss: 2.1258 - acc: 0.6390\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qYcaiuZn-ilI",
        "outputId": "412be295-5be9-4466-d934-bb7c4975a36f"
      },
      "source": [
        "from scipy import stats\n",
        "stats.describe(acc_arr)"
      ],
      "id": "qYcaiuZn-ilI",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DescribeResult(nobs=10, minmax=(0.5935, 0.7317), mean=0.64772, variance=0.0015455236, skewness=0.8875579237937927, kurtosis=0.24844141244357187)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-WTN7yqi_Im"
      },
      "source": [
        "### NIST High School"
      ],
      "id": "I-WTN7yqi_Im"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "545fdtHXfC9P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8524930e-5c4d-492e-d953-5ccf5fafedf4"
      },
      "source": [
        "images,labels=load_hsf_nist(4,55000)\n",
        "res.evaluate(images[10000:],labels[10000:])\n",
        "#preds = res.predict(images)\n",
        "#predicted_labels = get_argmax_labels(preds)\n",
        "#accuracy_score(labels, predicted_labels)"
      ],
      "id": "545fdtHXfC9P",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "45000/45000 [==============================] - 3s 75us/sample - loss: 0.0525 - acc: 0.9862\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.05250930038392944, 0.9862222]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tzkLBZtkgHAA",
        "outputId": "f6c5aa85-fb66-4eeb-e1c7-e0e2af1e38d0"
      },
      "source": [
        "images[10000:].shape"
      ],
      "id": "tzkLBZtkgHAA",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(45000, 28, 28, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJfsE5D8RYJe"
      },
      "source": [
        "### NIST Census MD"
      ],
      "id": "XJfsE5D8RYJe"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R8dWJu2kz6Ff",
        "outputId": "c24d8541-4c0f-4986-8471-3df210c63cbc"
      },
      "source": [
        "images,labels=load_hsf_nist(7,50000)\n",
        "res.evaluate(images,labels)"
      ],
      "id": "R8dWJu2kz6Ff",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50000/50000 [==============================] - 4s 74us/sample - loss: 0.0376 - acc: 0.9909\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.03760134117059639, 0.99086]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1bbja_4RccP"
      },
      "source": [
        "### NIST Census Field"
      ],
      "id": "r1bbja_4RccP"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ACo5aTDd0J3v",
        "outputId": "c46edc2b-05be-44e0-86d2-23fc42e3f97f"
      },
      "source": [
        "images,labels=load_hsf_nist(0,50000)\n",
        "res.evaluate(images,labels)"
      ],
      "id": "ACo5aTDd0J3v",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50000/50000 [==============================] - 4s 73us/sample - loss: 0.0643 - acc: 0.9851\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.0642632317849752, 0.98508]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wx8r-tf30g0k"
      },
      "source": [
        "### Synthetic data"
      ],
      "id": "Wx8r-tf30g0k"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTlfCqiu0jwA"
      },
      "source": [
        "### load generated samples\n",
        "generate = np.load(os.path.join('/content/drive/MyDrive/Internships/improved_wgan_training/models/wgan-gp_30Ksamples_100kiters', 'generated.npz'))\n",
        "labels = np.load(os.path.join('/content/drive/MyDrive/Internships/improved_wgan_training/models/wgan-gp_30Ksamples_100kiters', 'generated_labels.npz'))['labels']\n",
        "gen_imgs = generate['img_r01']\n",
        "gen_z = generate['noise']\n",
        "gen_feature = np.reshape(gen_imgs, [len(gen_imgs), -1])\n",
        "gen_feature = 2. * gen_feature - 1.\n",
        "gen_feature = gen_feature.reshape(100,28,28,1)"
      ],
      "id": "tTlfCqiu0jwA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EnWb5qML0mw2",
        "outputId": "be487a36-06c5-457b-9769-b2c98cd8327a"
      },
      "source": [
        "res.evaluate(gen_feature, labels)"
      ],
      "id": "EnWb5qML0mw2",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100/100 [==============================] - 0s 672us/sample - loss: 0.4471 - acc: 0.9400\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.4471098852157593, 0.94]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbsU5zT_86qM"
      },
      "source": [
        "predicted_labels = get_argmax_labels(res.predict(gen_feature))"
      ],
      "id": "QbsU5zT_86qM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bdD2bC3S8_Je",
        "outputId": "b7b5354a-2121-4466-df25-f24cc4bd3ecf"
      },
      "source": [
        "predicted_labels[90:100]"
      ],
      "id": "bdD2bC3S8_Je",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[8, 5, 6, 5, 2, 7, 6, 3, 7, 9]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hcmv9rB19BlW",
        "outputId": "82634483-9dd1-49d1-b8ba-298d856541ed"
      },
      "source": [
        "labels.tolist()[90:100]"
      ],
      "id": "Hcmv9rB19BlW",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[8, 5, 6, 5, 2, 7, 6, 3, 7, 9]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKuK2LvLbNwX"
      },
      "source": [
        "## Feature extraction"
      ],
      "id": "kKuK2LvLbNwX"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wjciZAPdGGJ"
      },
      "source": [
        "feature_extractor = Model(inputs=res.input, outputs=res.get_layer('flatten').output)"
      ],
      "id": "5wjciZAPdGGJ",
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-pfcZKR8euex",
        "outputId": "558f9b22-8b0b-4c2d-9090-f56d78be5a24"
      },
      "source": [
        "feature_clf = RandomForestClassifier()\n",
        "feature_clf.fit(feature_extractor.predict(X_train), y_train)"
      ],
      "id": "-pfcZKR8euex",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
              "                       criterion='gini', max_depth=None, max_features='auto',\n",
              "                       max_leaf_nodes=None, max_samples=None,\n",
              "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                       min_samples_leaf=1, min_samples_split=2,\n",
              "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
              "                       n_jobs=None, oob_score=False, random_state=0, verbose=0,\n",
              "                       warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZkNnqz2wyCFA",
        "outputId": "98eef378-4483-45eb-d240-bfe39cae79d0"
      },
      "source": [
        "y_pred_test = feature_clf.predict(feature_extractor.predict(X_test))\n",
        "accuracy_score(y_test, y_pred_test)"
      ],
      "id": "ZkNnqz2wyCFA",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9864"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y88T3EJ3ddh2"
      },
      "source": [
        "X_train_keras_features = keras_feature_extractor.predict(X_train_keras)\n",
        "X_train_keras_features = X_train_keras_features.reshape(X_train_keras_features.shape[0], X_train_keras_features.shape[-1])\n",
        "X_test_keras_features = keras_feature_extractor.predict(X_test_keras)\n",
        "X_test_keras_features = X_test_keras_features.reshape(X_test_keras_features.shape[0], X_test_keras_features.shape[-1])"
      ],
      "id": "y88T3EJ3ddh2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3LJUQzSaKTJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae8acf1c-9ea4-408a-8a28-aa3b2af4540c"
      },
      "source": [
        "feature_clf = RandomForestClassifier()\n",
        "feature_clf.fit(X_train_keras_features, y_train_keras)"
      ],
      "id": "_3LJUQzSaKTJ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
              "                       criterion='gini', max_depth=None, max_features='auto',\n",
              "                       max_leaf_nodes=None, max_samples=None,\n",
              "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                       min_samples_leaf=1, min_samples_split=2,\n",
              "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
              "                       n_jobs=None, oob_score=False, random_state=None,\n",
              "                       verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CoIjozbRaAuC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4ea1d7e-fe76-4e00-e556-88315b67522d"
      },
      "source": [
        "y_pred = feature_clf.predict(X_test_keras_features)\n",
        "accuracy_score(y_test, y_pred)"
      ],
      "id": "CoIjozbRaAuC",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8983"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gqzkz7KFymNp",
        "outputId": "5a978495-9156-46a5-f2e4-5cfde645c09e"
      },
      "source": [
        "X_test2,y_test2=load_hsf_nist(4,55000)\n",
        "y_pred_test2 = feature_clf.predict(feature_extractor.predict(X_test2))\n",
        "accuracy_score(y_test2, y_pred_test2)"
      ],
      "id": "Gqzkz7KFymNp",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9915818181818182"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlmOOCfA9eOW"
      },
      "source": [
        "## High-school or non high-school recognition"
      ],
      "id": "wlmOOCfA9eOW"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnDu_7IyyRpM"
      },
      "source": [
        "### Keras pre-trained resnet "
      ],
      "id": "xnDu_7IyyRpM"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZc8mYjU-yTp"
      },
      "source": [
        "resize_width = 32\n",
        "resize_height = 32\n",
        "X,y = load_high_school_vs_non(60000, resize_width, resize_height)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
        "X_test_keras = np.apply_along_axis(grey2RGB,-1,X_test).reshape(X_test.shape[0], resize_width, resize_height,3)\n",
        "X_train_keras = np.apply_along_axis(grey2RGB,-1,X_train).reshape(X_train.shape[0], resize_width, resize_height,3)"
      ],
      "id": "BZc8mYjU-yTp",
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMZ535ooyAHu"
      },
      "source": [
        "X_train_keras_features = keras_feature_extractor.predict(X_train_keras)\n",
        "X_train_keras_features = X_train_keras_features.reshape(X_train_keras_features.shape[0], X_train_keras_features.shape[-1])\n",
        "X_test_keras_features = keras_feature_extractor.predict(X_test_keras)\n",
        "X_test_keras_features = X_test_keras_features.reshape(X_test_keras_features.shape[0], X_test_keras_features.shape[-1])"
      ],
      "id": "wMZ535ooyAHu",
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nPcXOAkMyEXr",
        "outputId": "34a0a74a-dcae-4865-f412-800896fb0d79"
      },
      "source": [
        "feature_clf = RandomForestClassifier()\n",
        "feature_clf.fit(X_train_keras_features, y_train_keras)"
      ],
      "id": "nPcXOAkMyEXr",
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
              "                       criterion='gini', max_depth=None, max_features='auto',\n",
              "                       max_leaf_nodes=None, max_samples=None,\n",
              "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                       min_samples_leaf=1, min_samples_split=2,\n",
              "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
              "                       n_jobs=None, oob_score=False, random_state=None,\n",
              "                       verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hEWoRQcKyMk6",
        "outputId": "c857d989-e0c4-486c-9523-50cae0f18cc3"
      },
      "source": [
        "y_pred = feature_clf.predict(X_test_keras_features)\n",
        "accuracy_score(y_test, y_pred)"
      ],
      "id": "hEWoRQcKyMk6",
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7176666666666667"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppJNJ8EMya4t"
      },
      "source": [
        "### Own resnet"
      ],
      "id": "ppJNJ8EMya4t"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49L0vN6lygVn"
      },
      "source": [
        "X,y = load_high_school_vs_non(60000)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)"
      ],
      "id": "49L0vN6lygVn",
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzrNXwUX_OnI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6dd0aef-19be-4fb7-b421-d89af8bd7db6"
      },
      "source": [
        "res = get_resnet(binary_classification=True)\n",
        "history = res.fit(X_train, y_train, validation_split=0.1, verbose=2, epochs=15, batch_size=128, shuffle=False)"
      ],
      "id": "PzrNXwUX_OnI",
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 48600 samples, validate on 5400 samples\n",
            "Epoch 1/15\n",
            "48600/48600 - 7s - loss: 0.6349 - acc: 0.6529 - auc_2: 0.7016 - val_loss: 0.5852 - val_acc: 0.7044 - val_auc_2: 0.7691\n",
            "Epoch 2/15\n",
            "48600/48600 - 3s - loss: 0.5866 - acc: 0.6961 - auc_2: 0.7602 - val_loss: 0.5584 - val_acc: 0.7206 - val_auc_2: 0.7965\n",
            "Epoch 3/15\n",
            "48600/48600 - 3s - loss: 0.5656 - acc: 0.7144 - auc_2: 0.7825 - val_loss: 0.5516 - val_acc: 0.7230 - val_auc_2: 0.8154\n",
            "Epoch 4/15\n",
            "48600/48600 - 3s - loss: 0.5465 - acc: 0.7283 - auc_2: 0.7994 - val_loss: 0.5219 - val_acc: 0.7502 - val_auc_2: 0.8278\n",
            "Epoch 5/15\n",
            "48600/48600 - 3s - loss: 0.5286 - acc: 0.7420 - auc_2: 0.8151 - val_loss: 0.5125 - val_acc: 0.7493 - val_auc_2: 0.8379\n",
            "Epoch 6/15\n",
            "48600/48600 - 3s - loss: 0.5146 - acc: 0.7515 - auc_2: 0.8268 - val_loss: 0.5128 - val_acc: 0.7541 - val_auc_2: 0.8474\n",
            "Epoch 7/15\n",
            "48600/48600 - 3s - loss: 0.5012 - acc: 0.7613 - auc_2: 0.8373 - val_loss: 0.4860 - val_acc: 0.7698 - val_auc_2: 0.8513\n",
            "Epoch 8/15\n",
            "48600/48600 - 3s - loss: 0.4920 - acc: 0.7687 - auc_2: 0.8445 - val_loss: 0.5134 - val_acc: 0.7506 - val_auc_2: 0.8552\n",
            "Epoch 9/15\n",
            "48600/48600 - 3s - loss: 0.4831 - acc: 0.7727 - auc_2: 0.8506 - val_loss: 0.4902 - val_acc: 0.7617 - val_auc_2: 0.8554\n",
            "Epoch 10/15\n",
            "48600/48600 - 3s - loss: 0.4776 - acc: 0.7752 - auc_2: 0.8542 - val_loss: 0.4776 - val_acc: 0.7707 - val_auc_2: 0.8570\n",
            "Epoch 11/15\n",
            "48600/48600 - 3s - loss: 0.4731 - acc: 0.7801 - auc_2: 0.8578 - val_loss: 0.4650 - val_acc: 0.7817 - val_auc_2: 0.8646\n",
            "Epoch 12/15\n",
            "48600/48600 - 3s - loss: 0.4669 - acc: 0.7815 - auc_2: 0.8614 - val_loss: 0.4633 - val_acc: 0.7852 - val_auc_2: 0.8646\n",
            "Epoch 13/15\n",
            "48600/48600 - 3s - loss: 0.4626 - acc: 0.7858 - auc_2: 0.8643 - val_loss: 0.4589 - val_acc: 0.7854 - val_auc_2: 0.8672\n",
            "Epoch 14/15\n",
            "48600/48600 - 3s - loss: 0.4593 - acc: 0.7883 - auc_2: 0.8670 - val_loss: 0.4730 - val_acc: 0.7785 - val_auc_2: 0.8682\n",
            "Epoch 15/15\n",
            "48600/48600 - 3s - loss: 0.4561 - acc: 0.7905 - auc_2: 0.8688 - val_loss: 0.4954 - val_acc: 0.7580 - val_auc_2: 0.8636\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIH9W9AC_PDM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6eb0545-b8ba-4520-e821-2f4b31e2b59b"
      },
      "source": [
        "res.evaluate(X_test, y_test)"
      ],
      "id": "IIH9W9AC_PDM",
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6000/6000 [==============================] - 1s 141us/sample - loss: 0.5010 - acc: 0.7543 - auc_2: 0.8651\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.5009915000597636, 0.7543333, 0.8651493]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGgordPg_RaY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb2d2adb-a38e-4084-e739-f706f78b236a"
      },
      "source": [
        "images1,_ = load_hsf_nist(4,40000)\n",
        "images1 = images1[40000:]\n",
        "images2,_ = load_hsf_nist(7,10000)\n",
        "labels1 = np.array([1 for l in range(images1.shape[0])])\n",
        "labels2 = np.array([0 for l in range(images2.shape[0])])\n",
        "res.evaluate(np.concatenate((images1,images2)),np.concatenate((labels1,labels2)))"
      ],
      "id": "vGgordPg_RaY",
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 1s 142us/sample - loss: 1.3732 - acc: 0.3103 - auc_2: 0.0000e+00\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.3732483360290528, 0.3103, 0.0]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    }
  ]
}